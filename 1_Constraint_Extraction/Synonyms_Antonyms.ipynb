{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Constraints Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script extracts the synonym and antonym pairs from the WordNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import itertools\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Set the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/effylu/Desktop/Thesis\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Find the Synonym Pairs in the WordNet\n",
    "\n",
    "1. Find the all synsets in the WordNet.\n",
    "2. Find all pairs of the synonyms in each synsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n"
     ]
    }
   ],
   "source": [
    "print(wordnet.get_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Understand the WordNet\n",
    "\n",
    "1. \"lemma\" is the wordâ€™s morphological stem\n",
    "2. \"pos\" is one of the module attributes ADJ, ADJ_SAT, ADV, NOUN or VERB\n",
    "\n",
    "        n    NOUN\n",
    "        v    VERB\n",
    "        a    ADJECTIVE\n",
    "        s    ADJECTIVE SATELLITE\n",
    "        r    ADVERB \n",
    "    \n",
    "3. \"number\" is the sense number, counting from 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma('lower_jaw.n.01.lower_jaw')\n",
      "Lemma('lower_jaw.n.01.mandible')\n",
      "Lemma('lower_jaw.n.01.mandibula')\n",
      "Lemma('lower_jaw.n.01.mandibular_bone')\n",
      "Lemma('lower_jaw.n.01.submaxilla')\n",
      "Lemma('lower_jaw.n.01.lower_jawbone')\n",
      "Lemma('lower_jaw.n.01.jawbone')\n",
      "Lemma('lower_jaw.n.01.jowl')\n"
     ]
    }
   ],
   "source": [
    "synset_jawbone = wordnet.synset('jawbone.n.01').lemmas()\n",
    "for i in synset_jawbone:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lower_jaw.n.01.lower_jaw\n",
      "lower_jaw.n.01.mandible\n",
      "lower_jaw.n.01.mandibula\n",
      "lower_jaw.n.01.mandibular_bone\n",
      "lower_jaw.n.01.submaxilla\n",
      "lower_jaw.n.01.lower_jawbone\n",
      "lower_jaw.n.01.jawbone\n",
      "lower_jaw.n.01.jowl\n"
     ]
    }
   ],
   "source": [
    "synonym_jawbone = list()\n",
    "for i in synset_jawbone:\n",
    "    synonym_jawbone.append(str(i)[7:-2])\n",
    "for i in synonym_jawbone:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('lower_jaw.n.01.lower_jaw', 'lower_jaw.n.01.mandible')\n",
      "('lower_jaw.n.01.lower_jaw', 'lower_jaw.n.01.mandibula')\n",
      "('lower_jaw.n.01.lower_jaw', 'lower_jaw.n.01.mandibular_bone')\n",
      "('lower_jaw.n.01.lower_jaw', 'lower_jaw.n.01.submaxilla')\n",
      "('lower_jaw.n.01.lower_jaw', 'lower_jaw.n.01.lower_jawbone')\n",
      "('lower_jaw.n.01.lower_jaw', 'lower_jaw.n.01.jawbone')\n",
      "('lower_jaw.n.01.lower_jaw', 'lower_jaw.n.01.jowl')\n",
      "('lower_jaw.n.01.mandible', 'lower_jaw.n.01.mandibula')\n",
      "('lower_jaw.n.01.mandible', 'lower_jaw.n.01.mandibular_bone')\n",
      "('lower_jaw.n.01.mandible', 'lower_jaw.n.01.submaxilla')\n",
      "('lower_jaw.n.01.mandible', 'lower_jaw.n.01.lower_jawbone')\n",
      "('lower_jaw.n.01.mandible', 'lower_jaw.n.01.jawbone')\n",
      "('lower_jaw.n.01.mandible', 'lower_jaw.n.01.jowl')\n",
      "('lower_jaw.n.01.mandibula', 'lower_jaw.n.01.mandibular_bone')\n",
      "('lower_jaw.n.01.mandibula', 'lower_jaw.n.01.submaxilla')\n",
      "('lower_jaw.n.01.mandibula', 'lower_jaw.n.01.lower_jawbone')\n",
      "('lower_jaw.n.01.mandibula', 'lower_jaw.n.01.jawbone')\n",
      "('lower_jaw.n.01.mandibula', 'lower_jaw.n.01.jowl')\n",
      "('lower_jaw.n.01.mandibular_bone', 'lower_jaw.n.01.submaxilla')\n",
      "('lower_jaw.n.01.mandibular_bone', 'lower_jaw.n.01.lower_jawbone')\n",
      "('lower_jaw.n.01.mandibular_bone', 'lower_jaw.n.01.jawbone')\n",
      "('lower_jaw.n.01.mandibular_bone', 'lower_jaw.n.01.jowl')\n",
      "('lower_jaw.n.01.submaxilla', 'lower_jaw.n.01.lower_jawbone')\n",
      "('lower_jaw.n.01.submaxilla', 'lower_jaw.n.01.jawbone')\n",
      "('lower_jaw.n.01.submaxilla', 'lower_jaw.n.01.jowl')\n",
      "('lower_jaw.n.01.lower_jawbone', 'lower_jaw.n.01.jawbone')\n",
      "('lower_jaw.n.01.lower_jawbone', 'lower_jaw.n.01.jowl')\n",
      "('lower_jaw.n.01.jawbone', 'lower_jaw.n.01.jowl')\n"
     ]
    }
   ],
   "source": [
    "synpair_jawbone = list(itertools.combinations(synonym_jawbone, 2))\n",
    "for i in synpair_jawbone:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Noun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get all noun synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "synsets_n = list()\n",
    "for i in list(wordnet.all_synsets('n')):\n",
    "    synsets_n.append(i.name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82115"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(synsets_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cell.n.02\n",
      "causal_agent.n.01\n",
      "person.n.01\n",
      "animal.n.01\n",
      "plant.n.02\n"
     ]
    }
   ],
   "source": [
    "for i in range(15,20):\n",
    "    print(synsets_n[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function \"get_synpairs_n( )\" is used to get the synonym pairs for  a noun synset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_synpairs_n(synset):\n",
    "    synonym_str = list()\n",
    "    for i in synset:\n",
    "        synonym_str.append(str(i)[7:-2])\n",
    "    synonym_pair = list(itertools.combinations(synonym_str, 2))\n",
    "    return synonym_pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get all synonym pairs for all noun synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "synpair_n = list()\n",
    "for i in synsets_n:\n",
    "    synset = wordnet.synset(i).lemmas()\n",
    "    synpair_n.append(get_synpairs_n(synset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('plant.n.02.plant', 'plant.n.02.flora')\n",
      "('plant.n.02.plant', 'plant.n.02.plant_life')\n",
      "('plant.n.02.flora', 'plant.n.02.plant_life')\n"
     ]
    }
   ],
   "source": [
    "for i in synpair_n[19]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add score at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "synpairs_n = list()\n",
    "for i in synpair_n:\n",
    "    for j in i:\n",
    "        synpairs_n.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('animal.n.01.animate_being', 'animal.n.01.creature')\n",
      "('animal.n.01.animate_being', 'animal.n.01.fauna')\n",
      "('animal.n.01.beast', 'animal.n.01.brute')\n",
      "('animal.n.01.beast', 'animal.n.01.creature')\n",
      "('animal.n.01.beast', 'animal.n.01.fauna')\n",
      "('animal.n.01.brute', 'animal.n.01.creature')\n",
      "('animal.n.01.brute', 'animal.n.01.fauna')\n",
      "('animal.n.01.creature', 'animal.n.01.fauna')\n",
      "('plant.n.02.plant', 'plant.n.02.flora')\n",
      "('plant.n.02.plant', 'plant.n.02.plant_life')\n",
      "('plant.n.02.flora', 'plant.n.02.plant_life')\n",
      "('food.n.01.food', 'food.n.01.nutrient')\n",
      "('artifact.n.01.artifact', 'artifact.n.01.artefact')\n",
      "('cognition.n.01.cognition', 'cognition.n.01.knowledge')\n",
      "('cognition.n.01.cognition', 'cognition.n.01.noesis')\n",
      "('cognition.n.01.knowledge', 'cognition.n.01.noesis')\n",
      "('motivation.n.01.motivation', 'motivation.n.01.motive')\n",
      "('motivation.n.01.motivation', 'motivation.n.01.need')\n",
      "('motivation.n.01.motive', 'motivation.n.01.need')\n",
      "('shape.n.02.shape', 'shape.n.02.form')\n"
     ]
    }
   ],
   "source": [
    "for i in range(30,50):\n",
    "    print(synpairs_n[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107469\n"
     ]
    }
   ],
   "source": [
    "print(len(synpairs_n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Verb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get all verb synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "synsets_v = list()\n",
    "for i in list(wordnet.all_synsets('v')):\n",
    "    synsets_v.append(i.name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13767"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(synsets_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inhale.v.02\n",
      "pant.v.01\n",
      "cough.v.01\n",
      "hack.v.08\n",
      "expectorate.v.02\n"
     ]
    }
   ],
   "source": [
    "for i in range(15,20):\n",
    "    print(synsets_v[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function \"get_synpairs_v( )\" is used to get the synonym pairs for  a verb synset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_synpairs_v(synsets):\n",
    "    synonym_str = list()\n",
    "    for i in synsets:\n",
    "        synonym_str.append(str(i)[7:-2])\n",
    "    synonym_pair = list(itertools.combinations(synonym_str, 2))\n",
    "    return synonym_pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get all synonym pairs for all verb synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "synpair_v = list()\n",
    "for i in synsets_v:\n",
    "    synset = wordnet.synset(i).lemmas()\n",
    "    synpair_v.append(get_synpairs_v(synset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('expectorate.v.02.expectorate', 'expectorate.v.02.cough_up')\n",
      "('expectorate.v.02.expectorate', 'expectorate.v.02.cough_out')\n",
      "('expectorate.v.02.expectorate', 'expectorate.v.02.spit_up')\n",
      "('expectorate.v.02.expectorate', 'expectorate.v.02.spit_out')\n",
      "('expectorate.v.02.cough_up', 'expectorate.v.02.cough_out')\n",
      "('expectorate.v.02.cough_up', 'expectorate.v.02.spit_up')\n",
      "('expectorate.v.02.cough_up', 'expectorate.v.02.spit_out')\n",
      "('expectorate.v.02.cough_out', 'expectorate.v.02.spit_up')\n",
      "('expectorate.v.02.cough_out', 'expectorate.v.02.spit_out')\n",
      "('expectorate.v.02.spit_up', 'expectorate.v.02.spit_out')\n"
     ]
    }
   ],
   "source": [
    "for i in synpair_v[19]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add score at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "synpairs_v = list()\n",
    "for i in synpair_v:\n",
    "    for j in i:\n",
    "        synpairs_v.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('pant.v.01.pant', 'pant.v.01.heave')\n",
      "('pant.v.01.puff', 'pant.v.01.gasp')\n",
      "('pant.v.01.puff', 'pant.v.01.heave')\n",
      "('pant.v.01.gasp', 'pant.v.01.heave')\n",
      "('hack.v.08.hack', 'hack.v.08.whoop')\n",
      "('expectorate.v.02.expectorate', 'expectorate.v.02.cough_up')\n",
      "('expectorate.v.02.expectorate', 'expectorate.v.02.cough_out')\n",
      "('expectorate.v.02.expectorate', 'expectorate.v.02.spit_up')\n",
      "('expectorate.v.02.expectorate', 'expectorate.v.02.spit_out')\n",
      "('expectorate.v.02.cough_up', 'expectorate.v.02.cough_out')\n",
      "('expectorate.v.02.cough_up', 'expectorate.v.02.spit_up')\n",
      "('expectorate.v.02.cough_up', 'expectorate.v.02.spit_out')\n",
      "('expectorate.v.02.cough_out', 'expectorate.v.02.spit_up')\n",
      "('expectorate.v.02.cough_out', 'expectorate.v.02.spit_out')\n",
      "('expectorate.v.02.spit_up', 'expectorate.v.02.spit_out')\n",
      "('puff.v.08.puff', 'puff.v.08.huff')\n",
      "('puff.v.08.puff', 'puff.v.08.chuff')\n",
      "('puff.v.08.huff', 'puff.v.08.chuff')\n",
      "('sniff.v.02.sniff', 'sniff.v.02.sniffle')\n",
      "('blink.v.01.blink', 'blink.v.01.wink')\n"
     ]
    }
   ],
   "source": [
    "for i in range(25,45):\n",
    "    print(synpairs_v[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24577\n"
     ]
    }
   ],
   "source": [
    "print(len(synpairs_v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Adjectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get all adjective synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "synsets_a = list()\n",
    "for i in list(wordnet.all_synsets('a')):\n",
    "    synsets_a.append(i.name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18156"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(synsets_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abridged.a.01\n",
      "cut.s.03\n",
      "half-length.s.02\n",
      "potted.s.03\n",
      "unabridged.a.01\n"
     ]
    }
   ],
   "source": [
    "for i in range(15,20):\n",
    "    print(synsets_a[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function \"get_synpairs_a( )\" is used to get the synonym pairs for  an adjectives synset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_synpairs_a(synsets):\n",
    "    synonym_str = list()\n",
    "    for i in synsets:\n",
    "        synonym_str.append(str(i)[7:-2])\n",
    "    synonym_pair = list(itertools.combinations(synonym_str, 2))\n",
    "    return synonym_pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get all synonym pairs for all adjective synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "synpair_a = list()\n",
    "for i in synsets_a:\n",
    "    synset = wordnet.synset(i).lemmas()\n",
    "    synpair_a.append(get_synpairs_a(synset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('cut.s.03.cut', 'cut.s.03.shortened')\n"
     ]
    }
   ],
   "source": [
    "for i in synpair_a[16]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add score at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "synpairs_a = list()\n",
    "for i in synpair_a:\n",
    "    for j in i:\n",
    "        synpairs_a.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('abaxial.a.01.abaxial', 'abaxial.a.01.dorsal')\n",
      "('adaxial.a.01.adaxial', 'adaxial.a.01.ventral')\n",
      "('abducent.a.01.abducent', 'abducent.a.01.abducting')\n",
      "('adducent.a.01.adducent', 'adducent.a.01.adductive')\n",
      "('adducent.a.01.adducent', 'adducent.a.01.adducting')\n",
      "('adducent.a.01.adductive', 'adducent.a.01.adducting')\n",
      "('emergent.s.02.emergent', 'emergent.s.02.emerging')\n",
      "('cut.s.03.cut', 'cut.s.03.shortened')\n",
      "('full-length.s.02.full-length', 'full-length.s.02.uncut')\n",
      "('implicit.s.02.implicit', 'implicit.s.02.unquestioning')\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(synpairs_a[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22730\n"
     ]
    }
   ],
   "source": [
    "print(len(synpairs_a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. The total amount of the synonym pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154776\n"
     ]
    }
   ],
   "source": [
    "print(len(synpairs_n)+len(synpairs_v)+len(synpairs_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "synpairs = list()\n",
    "for i in synpairs_n:\n",
    "    synpairs.append(i)\n",
    "for j in synpairs_v:\n",
    "    synpairs.append(j)\n",
    "for k in synpairs_a:\n",
    "    synpairs.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('synpairs.txt', 'w') as f:\n",
    "    for i in synpairs:\n",
    "        f.write(\"%s\\n\" % str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Find the Antonym Pairs in the WordNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Noun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function \"get_antonym_synset_n( )\" is used to get the antonym synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_antonym_synset_n(synset):\n",
    "    syn_ant_set = list()\n",
    "    for l in wordnet.synset(synset).lemmas():\n",
    "        if l.antonyms():\n",
    "            antset = str(l.antonyms()[0].synset())[8:-2]\n",
    "            antonym_synset = [synset, antset]\n",
    "            syn_ant_set.append(antonym_synset)\n",
    "            syn_ant_set = list(syn_ant_set for syn_ant_set,_ in itertools.groupby(syn_ant_set))\n",
    "    return syn_ant_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get all noun antonym synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "antonym_synset_n = list()\n",
    "for i in synsets_n:\n",
    "    antonym_synset_pair = get_antonym_synset_n(i)\n",
    "    if antonym_synset_pair != []:\n",
    "        antonym_synset_n.append(antonym_synset_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "antsyn_n = list()\n",
    "for i in antonym_synset_n:\n",
    "    for j in i:\n",
    "        antsyn_n.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural_object.n.01', 'artifact.n.01']\n",
      "['artifact.n.01', 'natural_object.n.01']\n",
      "['overachievement.n.01', 'underachievement.n.01']\n",
      "['underachievement.n.01', 'overachievement.n.01']\n",
      "['appearance.n.05', 'disappearance.n.01']\n",
      "['disappearance.n.01', 'appearance.n.05']\n",
      "['retreat.n.07', 'progress.n.02']\n",
      "['debarkation.n.01', 'boarding.n.01']\n",
      "['boarding.n.01', 'debarkation.n.01']\n",
      "['passing.n.07', 'failing.n.02']\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(antsyn_n[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = wordnet.synset('conformity.n.02').lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma('conformity.n.02.conformity')\n",
      "Synset('nonconformity.n.04')\n",
      "Lemma('conformity.n.02.conformation')\n",
      "no\n",
      "Lemma('conformity.n.02.compliance')\n",
      "Synset('disobedience.n.01')\n",
      "Lemma('conformity.n.02.abidance')\n",
      "no\n"
     ]
    }
   ],
   "source": [
    "for i in a:\n",
    "    print(i)\n",
    "    try:\n",
    "        print(i.antonyms()[0].synset())\n",
    "    except:\n",
    "        print('no')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sort the antonym synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in antsyn_n:\n",
    "    i.sort()\n",
    "antsyn_n.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abience.n.01', 'adience.n.01']\n",
      "['abience.n.01', 'adience.n.01']\n",
      "['ability.n.01', 'inability.n.02']\n",
      "['ability.n.01', 'inability.n.02']\n",
      "['ability.n.02', 'inability.n.01']\n",
      "['ability.n.02', 'inability.n.01']\n",
      "['abnormality.n.01', 'normality.n.01']\n",
      "['abnormality.n.01', 'normality.n.01']\n",
      "['abruptness.n.02', 'gradualness.n.01']\n",
      "['abruptness.n.02', 'gradualness.n.01']\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(antsyn_n[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "antsyn_n = list(antsyn_n for antsyn_n,_ in itertools.groupby(antsyn_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abience.n.01', 'adience.n.01']\n",
      "['ability.n.01', 'inability.n.02']\n",
      "['ability.n.02', 'inability.n.01']\n",
      "['abnormality.n.01', 'normality.n.01']\n",
      "['abruptness.n.02', 'gradualness.n.01']\n",
      "['absence.n.01', 'presence.n.01']\n",
      "['absence.n.02', 'presence.n.06']\n",
      "['absorbency.n.01', 'nonabsorbency.n.01']\n",
      "['abstainer.n.02', 'drinker.n.02']\n",
      "['abstractness.n.01', 'concreteness.n.01']\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(antsyn_n[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "969"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(antsyn_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function \"get_antpairs_n( )\" is used to get all antonym pairs of an antonym synset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_antpairs_n(antonym):\n",
    "    syn_1 = wordnet.synset(antonym[0]).lemmas()\n",
    "    synset_1 = list()\n",
    "    for i in syn_1:\n",
    "        synset_1.append(str(i)[7:-2])\n",
    "    syn_2 = wordnet.synset(antonym[1]).lemmas()\n",
    "    synset_2 = list()\n",
    "    for i in syn_2:\n",
    "        synset_2.append(str(i)[7:-2])\n",
    "    ant_pair_n = list(itertools.product(synset_1, synset_2))\n",
    "    return ant_pair_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('good.n.02.good', 'evil.n.03.evil')\n",
      "('good.n.02.good', 'evil.n.03.evilness')\n",
      "('good.n.02.goodness', 'evil.n.03.evil')\n",
      "('good.n.02.goodness', 'evil.n.03.evilness')\n"
     ]
    }
   ],
   "source": [
    "for i in get_antpairs_n(['good.n.02', 'evil.n.03']):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get all antonym pairs for all antonym synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "antpair_n = list()\n",
    "for i in antsyn_n:\n",
    "    antpair_n.append(get_antpairs_n(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "antpairs_n = list()\n",
    "for i in antpair_n:\n",
    "    for j in i:\n",
    "        antpairs_n.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('abience.n.01.abience', 'adience.n.01.adience')\n",
      "('ability.n.01.ability', 'inability.n.02.inability')\n",
      "('ability.n.01.ability', 'inability.n.02.unfitness')\n",
      "('ability.n.02.ability', 'inability.n.01.inability')\n",
      "('ability.n.02.power', 'inability.n.01.inability')\n",
      "('abnormality.n.01.abnormality', 'normality.n.01.normality')\n",
      "('abnormality.n.01.abnormality', 'normality.n.01.normalcy')\n",
      "('abnormality.n.01.abnormalcy', 'normality.n.01.normality')\n",
      "('abnormality.n.01.abnormalcy', 'normality.n.01.normalcy')\n",
      "('abruptness.n.02.abruptness', 'gradualness.n.01.gradualness')\n",
      "('abruptness.n.02.abruptness', 'gradualness.n.01.gentleness')\n",
      "('abruptness.n.02.precipitousness', 'gradualness.n.01.gradualness')\n",
      "('abruptness.n.02.precipitousness', 'gradualness.n.01.gentleness')\n",
      "('abruptness.n.02.steepness', 'gradualness.n.01.gradualness')\n",
      "('abruptness.n.02.steepness', 'gradualness.n.01.gentleness')\n",
      "('absence.n.01.absence', 'presence.n.01.presence')\n",
      "('absence.n.02.absence', 'presence.n.06.presence')\n",
      "('absorbency.n.01.absorbency', 'nonabsorbency.n.01.nonabsorbency')\n",
      "('abstainer.n.02.abstainer', 'drinker.n.02.drinker')\n",
      "('abstainer.n.02.abstainer', 'drinker.n.02.imbiber')\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(antpairs_n[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3184\n"
     ]
    }
   ],
   "source": [
    "print(len(antpairs_n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Verb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function \"get_antonym_synset_v( )\" is used to get the antonym synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_antonym_synset_v(synset):\n",
    "    syn_ant_set = list()\n",
    "    for l in wordnet.synset(synset).lemmas():\n",
    "        if l.antonyms():\n",
    "            antset = str(l.antonyms()[0].synset())[8:-2]\n",
    "            antonym_synset = [synset, antset]\n",
    "            syn_ant_set.append(antonym_synset)\n",
    "            syn_ant_set = list(syn_ant_set for syn_ant_set,_ in itertools.groupby(syn_ant_set))\n",
    "    return syn_ant_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get all verb antonym synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "antonym_synset_v = list()\n",
    "for i in synsets_v:\n",
    "    antonym_synset_pair = get_antonym_synset_v(i)\n",
    "    if antonym_synset_pair != []:\n",
    "        antonym_synset_v.append(antonym_synset_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "antsyn_v = list()\n",
    "for i in antonym_synset_v:\n",
    "    for j in i:\n",
    "        antsyn_v.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['exhale.v.01', 'inhale.v.02']\n",
      "['inhale.v.02', 'exhale.v.01']\n",
      "['rest.v.05', 'be_active.v.01']\n",
      "['be_active.v.01', 'rest.v.05']\n",
      "['sleep.v.01', 'wake.v.01']\n",
      "['hibernate.v.01', 'estivate.v.01']\n",
      "['estivate.v.01', 'hibernate.v.01']\n",
      "['fall_asleep.v.01', 'wake_up.v.02']\n",
      "['go_to_bed.v.01', 'get_up.v.02']\n",
      "['get_up.v.02', 'go_to_bed.v.01']\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(antsyn_v[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sort the antonym synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in antsyn_v:\n",
    "    i.sort()\n",
    "antsyn_v.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abduct.v.02', 'adduct.v.01']\n",
      "['abduct.v.02', 'adduct.v.01']\n",
      "['abolish.v.01', 'establish.v.01']\n",
      "['abolish.v.01', 'establish.v.01']\n",
      "['abridge.v.01', 'elaborate.v.01']\n",
      "['abridge.v.01', 'elaborate.v.01']\n",
      "['absolve.v.02', 'blame.v.01']\n",
      "['absolve.v.02', 'blame.v.01']\n",
      "['absorb.v.06', 'emit.v.02']\n",
      "['absorb.v.06', 'emit.v.02']\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(antsyn_v[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "antsyn_v = list(antsyn_v for antsyn_v,_ in itertools.groupby(antsyn_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abduct.v.02', 'adduct.v.01']\n",
      "['abolish.v.01', 'establish.v.01']\n",
      "['abridge.v.01', 'elaborate.v.01']\n",
      "['absolve.v.02', 'blame.v.01']\n",
      "['absorb.v.06', 'emit.v.02']\n",
      "['abstain.v.02', 'consume.v.02']\n",
      "['accelerate.v.01', 'decelerate.v.01']\n",
      "['accelerate.v.02', 'decelerate.v.02']\n",
      "['accept.v.01', 'reject.v.01']\n",
      "['accept.v.02', 'refuse.v.02']\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(antsyn_v[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "505"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(antsyn_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function \"get_antpairs_v( )\" is used to get all antonym pairs of an antonym synset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_antpairs_v(antonym):\n",
    "    syn_1 = wordnet.synset(antonym[0]).lemmas()\n",
    "    synset_1 = list()\n",
    "    for i in syn_1:\n",
    "        synset_1.append(str(i)[7:-2])\n",
    "    syn_2 = wordnet.synset(antonym[1]).lemmas()\n",
    "    synset_2 = list()\n",
    "    for i in syn_2:\n",
    "        synset_2.append(str(i)[7:-2])\n",
    "    ant_pair = list(itertools.product(synset_1, synset_2))\n",
    "    return ant_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('abridge.v.01.abridge', 'elaborate.v.01.elaborate')\n",
      "('abridge.v.01.abridge', 'elaborate.v.01.lucubrate')\n",
      "('abridge.v.01.abridge', 'elaborate.v.01.expatiate')\n",
      "('abridge.v.01.abridge', 'elaborate.v.01.exposit')\n",
      "('abridge.v.01.abridge', 'elaborate.v.01.enlarge')\n",
      "('abridge.v.01.abridge', 'elaborate.v.01.flesh_out')\n",
      "('abridge.v.01.abridge', 'elaborate.v.01.expand')\n",
      "('abridge.v.01.abridge', 'elaborate.v.01.expound')\n",
      "('abridge.v.01.abridge', 'elaborate.v.01.dilate')\n",
      "('abridge.v.01.foreshorten', 'elaborate.v.01.elaborate')\n",
      "('abridge.v.01.foreshorten', 'elaborate.v.01.lucubrate')\n",
      "('abridge.v.01.foreshorten', 'elaborate.v.01.expatiate')\n",
      "('abridge.v.01.foreshorten', 'elaborate.v.01.exposit')\n",
      "('abridge.v.01.foreshorten', 'elaborate.v.01.enlarge')\n",
      "('abridge.v.01.foreshorten', 'elaborate.v.01.flesh_out')\n",
      "('abridge.v.01.foreshorten', 'elaborate.v.01.expand')\n",
      "('abridge.v.01.foreshorten', 'elaborate.v.01.expound')\n",
      "('abridge.v.01.foreshorten', 'elaborate.v.01.dilate')\n",
      "('abridge.v.01.abbreviate', 'elaborate.v.01.elaborate')\n",
      "('abridge.v.01.abbreviate', 'elaborate.v.01.lucubrate')\n",
      "('abridge.v.01.abbreviate', 'elaborate.v.01.expatiate')\n",
      "('abridge.v.01.abbreviate', 'elaborate.v.01.exposit')\n",
      "('abridge.v.01.abbreviate', 'elaborate.v.01.enlarge')\n",
      "('abridge.v.01.abbreviate', 'elaborate.v.01.flesh_out')\n",
      "('abridge.v.01.abbreviate', 'elaborate.v.01.expand')\n",
      "('abridge.v.01.abbreviate', 'elaborate.v.01.expound')\n",
      "('abridge.v.01.abbreviate', 'elaborate.v.01.dilate')\n",
      "('abridge.v.01.shorten', 'elaborate.v.01.elaborate')\n",
      "('abridge.v.01.shorten', 'elaborate.v.01.lucubrate')\n",
      "('abridge.v.01.shorten', 'elaborate.v.01.expatiate')\n",
      "('abridge.v.01.shorten', 'elaborate.v.01.exposit')\n",
      "('abridge.v.01.shorten', 'elaborate.v.01.enlarge')\n",
      "('abridge.v.01.shorten', 'elaborate.v.01.flesh_out')\n",
      "('abridge.v.01.shorten', 'elaborate.v.01.expand')\n",
      "('abridge.v.01.shorten', 'elaborate.v.01.expound')\n",
      "('abridge.v.01.shorten', 'elaborate.v.01.dilate')\n",
      "('abridge.v.01.cut', 'elaborate.v.01.elaborate')\n",
      "('abridge.v.01.cut', 'elaborate.v.01.lucubrate')\n",
      "('abridge.v.01.cut', 'elaborate.v.01.expatiate')\n",
      "('abridge.v.01.cut', 'elaborate.v.01.exposit')\n",
      "('abridge.v.01.cut', 'elaborate.v.01.enlarge')\n",
      "('abridge.v.01.cut', 'elaborate.v.01.flesh_out')\n",
      "('abridge.v.01.cut', 'elaborate.v.01.expand')\n",
      "('abridge.v.01.cut', 'elaborate.v.01.expound')\n",
      "('abridge.v.01.cut', 'elaborate.v.01.dilate')\n",
      "('abridge.v.01.contract', 'elaborate.v.01.elaborate')\n",
      "('abridge.v.01.contract', 'elaborate.v.01.lucubrate')\n",
      "('abridge.v.01.contract', 'elaborate.v.01.expatiate')\n",
      "('abridge.v.01.contract', 'elaborate.v.01.exposit')\n",
      "('abridge.v.01.contract', 'elaborate.v.01.enlarge')\n",
      "('abridge.v.01.contract', 'elaborate.v.01.flesh_out')\n",
      "('abridge.v.01.contract', 'elaborate.v.01.expand')\n",
      "('abridge.v.01.contract', 'elaborate.v.01.expound')\n",
      "('abridge.v.01.contract', 'elaborate.v.01.dilate')\n",
      "('abridge.v.01.reduce', 'elaborate.v.01.elaborate')\n",
      "('abridge.v.01.reduce', 'elaborate.v.01.lucubrate')\n",
      "('abridge.v.01.reduce', 'elaborate.v.01.expatiate')\n",
      "('abridge.v.01.reduce', 'elaborate.v.01.exposit')\n",
      "('abridge.v.01.reduce', 'elaborate.v.01.enlarge')\n",
      "('abridge.v.01.reduce', 'elaborate.v.01.flesh_out')\n",
      "('abridge.v.01.reduce', 'elaborate.v.01.expand')\n",
      "('abridge.v.01.reduce', 'elaborate.v.01.expound')\n",
      "('abridge.v.01.reduce', 'elaborate.v.01.dilate')\n"
     ]
    }
   ],
   "source": [
    "for i in get_antpairs_n(['abridge.v.01', 'elaborate.v.01']):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get all antonym pairs for all antonym synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "antpair_v = list()\n",
    "for i in antsyn_v:\n",
    "    antpair_v.append(get_antpairs_v(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "antpairs_v = list()\n",
    "for i in antpair_v:\n",
    "    for j in i:\n",
    "        antpairs_v.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('abridge.v.01.shorten', 'elaborate.v.01.enlarge')\n",
      "('abridge.v.01.shorten', 'elaborate.v.01.flesh_out')\n",
      "('abridge.v.01.shorten', 'elaborate.v.01.expand')\n",
      "('abridge.v.01.shorten', 'elaborate.v.01.expound')\n",
      "('abridge.v.01.shorten', 'elaborate.v.01.dilate')\n",
      "('abridge.v.01.cut', 'elaborate.v.01.elaborate')\n",
      "('abridge.v.01.cut', 'elaborate.v.01.lucubrate')\n",
      "('abridge.v.01.cut', 'elaborate.v.01.expatiate')\n",
      "('abridge.v.01.cut', 'elaborate.v.01.exposit')\n",
      "('abridge.v.01.cut', 'elaborate.v.01.enlarge')\n",
      "('abridge.v.01.cut', 'elaborate.v.01.flesh_out')\n",
      "('abridge.v.01.cut', 'elaborate.v.01.expand')\n",
      "('abridge.v.01.cut', 'elaborate.v.01.expound')\n",
      "('abridge.v.01.cut', 'elaborate.v.01.dilate')\n",
      "('abridge.v.01.contract', 'elaborate.v.01.elaborate')\n",
      "('abridge.v.01.contract', 'elaborate.v.01.lucubrate')\n",
      "('abridge.v.01.contract', 'elaborate.v.01.expatiate')\n",
      "('abridge.v.01.contract', 'elaborate.v.01.exposit')\n",
      "('abridge.v.01.contract', 'elaborate.v.01.enlarge')\n",
      "('abridge.v.01.contract', 'elaborate.v.01.flesh_out')\n",
      "('abridge.v.01.contract', 'elaborate.v.01.expand')\n",
      "('abridge.v.01.contract', 'elaborate.v.01.expound')\n",
      "('abridge.v.01.contract', 'elaborate.v.01.dilate')\n",
      "('abridge.v.01.reduce', 'elaborate.v.01.elaborate')\n",
      "('abridge.v.01.reduce', 'elaborate.v.01.lucubrate')\n",
      "('abridge.v.01.reduce', 'elaborate.v.01.expatiate')\n",
      "('abridge.v.01.reduce', 'elaborate.v.01.exposit')\n",
      "('abridge.v.01.reduce', 'elaborate.v.01.enlarge')\n",
      "('abridge.v.01.reduce', 'elaborate.v.01.flesh_out')\n",
      "('abridge.v.01.reduce', 'elaborate.v.01.expand')\n",
      "('abridge.v.01.reduce', 'elaborate.v.01.expound')\n",
      "('abridge.v.01.reduce', 'elaborate.v.01.dilate')\n",
      "('absolve.v.02.absolve', 'blame.v.01.blame')\n",
      "('absolve.v.02.absolve', 'blame.v.01.fault')\n",
      "('absolve.v.02.justify', 'blame.v.01.blame')\n",
      "('absolve.v.02.justify', 'blame.v.01.fault')\n",
      "('absolve.v.02.free', 'blame.v.01.blame')\n",
      "('absolve.v.02.free', 'blame.v.01.fault')\n",
      "('absorb.v.06.absorb', 'emit.v.02.emit')\n",
      "('absorb.v.06.absorb', 'emit.v.02.give_out')\n"
     ]
    }
   ],
   "source": [
    "for i in range(40,80):\n",
    "    print(antpairs_v[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3307\n"
     ]
    }
   ],
   "source": [
    "print(len(antpairs_v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Adjective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function \"get_antonym_synset_a( )\" is used to get the antonym synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_antonym_synset_a(synset):\n",
    "    syn_ant_set = list()\n",
    "    for l in wordnet.synset(synset).lemmas():\n",
    "        if l.antonyms():\n",
    "            antset = str(l.antonyms()[0].synset())[8:-2]\n",
    "            antonym_synset = [synset, antset]\n",
    "            syn_ant_set.append(antonym_synset)\n",
    "            syn_ant_set = list(syn_ant_set for syn_ant_set,_ in itertools.groupby(syn_ant_set))\n",
    "    return syn_ant_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get all adjective antonym synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "antonym_synset_a = list()\n",
    "for i in synsets_a:\n",
    "    antonym_synset_pair = get_antonym_synset_a(i)\n",
    "    if antonym_synset_pair != []:\n",
    "        antonym_synset_a.append(antonym_synset_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "antsyn_a = list()\n",
    "for i in antonym_synset_a:\n",
    "    for j in i:\n",
    "        antsyn_a.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['able.a.01', 'unable.a.01']\n",
      "['unable.a.01', 'able.a.01']\n",
      "['abaxial.a.01', 'adaxial.a.01']\n",
      "['adaxial.a.01', 'abaxial.a.01']\n",
      "['acroscopic.a.01', 'basiscopic.a.01']\n",
      "['basiscopic.a.01', 'acroscopic.a.01']\n",
      "['abducent.a.01', 'adducent.a.01']\n",
      "['adducent.a.01', 'abducent.a.01']\n",
      "['nascent.a.01', 'dying.a.01']\n",
      "['dying.a.01', 'nascent.a.01']\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(antsyn_a[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sort the antonym synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in antsyn_a:\n",
    "    i.sort()\n",
    "antsyn_a.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a_la_carte.a.01', \"table_d'hote.a.01\"]\n",
      "['a_la_carte.a.01', \"table_d'hote.a.01\"]\n",
      "['a_posteriori.a.01', 'a_priori.a.01']\n",
      "['a_posteriori.a.01', 'a_priori.a.01']\n",
      "['abactinal.a.01', 'actinal.a.01']\n",
      "['abactinal.a.01', 'actinal.a.01']\n",
      "['abaxial.a.01', 'adaxial.a.01']\n",
      "['abaxial.a.01', 'adaxial.a.01']\n",
      "['abducent.a.01', 'adducent.a.01']\n",
      "['abducent.a.01', 'adducent.a.01']\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(antsyn_a[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "antsyn_a = list(antsyn_a for antsyn_a,_ in itertools.groupby(antsyn_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a_la_carte.a.01', \"table_d'hote.a.01\"]\n",
      "['a_posteriori.a.01', 'a_priori.a.01']\n",
      "['abactinal.a.01', 'actinal.a.01']\n",
      "['abaxial.a.01', 'adaxial.a.01']\n",
      "['abducent.a.01', 'adducent.a.01']\n",
      "['abient.a.01', 'adient.a.01']\n",
      "['able.a.01', 'unable.a.01']\n",
      "['abnormal.a.01', 'normal.a.01']\n",
      "['abnormal.a.02', 'normal.a.03']\n",
      "['aboral.a.01', 'oral.a.03']\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(antsyn_a[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1947"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(antsyn_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function \"get_antpairs_a( )\" is used to get all antonym pairs of an antonym synset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_antpairs_a(antonym):\n",
    "    syn_1 = wordnet.synset(antonym[0]).lemmas()\n",
    "    synset_1 = list()\n",
    "    for i in syn_1:\n",
    "        synset_1.append(str(i)[7:-2])\n",
    "    syn_2 = wordnet.synset(antonym[1]).lemmas()\n",
    "    synset_2 = list()\n",
    "    for i in syn_2:\n",
    "        synset_2.append(str(i)[7:-2])\n",
    "    ant_pair = list(itertools.product(synset_1, synset_2))\n",
    "    return ant_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('abnormal.a.01.abnormal', 'normal.a.01.normal')\n",
      "('abnormal.a.01.unnatural', 'normal.a.01.normal')\n"
     ]
    }
   ],
   "source": [
    "for i in get_antpairs_n(['abnormal.a.01', 'normal.a.01']):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get all antonym pairs for all antonym synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "antpair_a = list()\n",
    "for i in antsyn_a:\n",
    "    antpair_a.append(get_antpairs_a(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "antpairs_a = list()\n",
    "for i in antpair_a:\n",
    "    for j in i:\n",
    "        antpairs_a.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a_la_carte.a.01.a_la_carte', \"table_d'hote.a.01.table_d'hote\")\n",
      "('a_la_carte.a.01.a_la_carte', \"table_d'hote.a.01.prix_fixe\")\n",
      "('a_posteriori.a.01.a_posteriori', 'a_priori.a.01.a_priori')\n",
      "('abactinal.a.01.abactinal', 'actinal.a.01.actinal')\n",
      "('abaxial.a.01.abaxial', 'adaxial.a.01.adaxial')\n",
      "('abaxial.a.01.abaxial', 'adaxial.a.01.ventral')\n",
      "('abaxial.a.01.dorsal', 'adaxial.a.01.adaxial')\n",
      "('abaxial.a.01.dorsal', 'adaxial.a.01.ventral')\n",
      "('abducent.a.01.abducent', 'adducent.a.01.adducent')\n",
      "('abducent.a.01.abducent', 'adducent.a.01.adductive')\n",
      "('abducent.a.01.abducent', 'adducent.a.01.adducting')\n",
      "('abducent.a.01.abducting', 'adducent.a.01.adducent')\n",
      "('abducent.a.01.abducting', 'adducent.a.01.adductive')\n",
      "('abducent.a.01.abducting', 'adducent.a.01.adducting')\n",
      "('abient.a.01.abient', 'adient.a.01.adient')\n",
      "('able.a.01.able', 'unable.a.01.unable')\n",
      "('abnormal.a.01.abnormal', 'normal.a.01.normal')\n",
      "('abnormal.a.01.unnatural', 'normal.a.01.normal')\n",
      "('abnormal.a.02.abnormal', 'normal.a.03.normal')\n",
      "('aboral.a.01.aboral', 'oral.a.03.oral')\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(antpairs_a[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3486\n"
     ]
    }
   ],
   "source": [
    "print(len(antpairs_a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. The total amount of the antonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9977\n"
     ]
    }
   ],
   "source": [
    "print(len(antpairs_n)+len(antpairs_v)+len(antpairs_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "antpairs = list()\n",
    "for i in antpairs_n:\n",
    "    antpairs.append(i)\n",
    "for j in antpairs_v:\n",
    "    antpairs.append(j)\n",
    "for k in antpairs_a:\n",
    "    antpairs.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('antpairs.txt', 'w') as f:\n",
    "    for i in antpairs:\n",
    "        f.write(\"%s\\n\" % str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Find the Hypernyms Pairs in the WordNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Noun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function \"get_hypernym_synset_n( )\" is used to get the hypernym synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_hypernym_synset_n(synset):\n",
    "    syn_hyp_set = list()\n",
    "    if wordnet.synset(synset).hypernyms():\n",
    "        for i in wordnet.synset(synset).hypernyms():\n",
    "            hypset = str(i)[8:-2]\n",
    "            hypernym_synset = [synset, hypset]\n",
    "            syn_hyp_set.append(hypernym_synset)\n",
    "            syn_hyp_set = list(syn_hyp_set for syn_hyp_set,_ in itertools.groupby(syn_hyp_set))\n",
    "    return syn_hyp_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get all noun hypernym synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hypernym_synset_n = list()\n",
    "for i in synsets_n:\n",
    "    hypernym_synset_pair = get_hypernym_synset_n(i)\n",
    "    if hypernym_synset_pair != []:\n",
    "        hypernym_synset_n.append(hypernym_synset_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hypsyn_n = list()\n",
    "for i in hypernym_synset_n:\n",
    "    for j in i:\n",
    "        hypsyn_n.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['physical_entity.n.01', 'entity.n.01']\n",
      "['abstraction.n.06', 'entity.n.01']\n",
      "['thing.n.12', 'physical_entity.n.01']\n",
      "['object.n.01', 'physical_entity.n.01']\n",
      "['whole.n.02', 'object.n.01']\n",
      "['congener.n.03', 'whole.n.02']\n",
      "['living_thing.n.01', 'whole.n.02']\n",
      "['organism.n.01', 'living_thing.n.01']\n",
      "['benthos.n.02', 'organism.n.01']\n",
      "['dwarf.n.03', 'organism.n.01']\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(hypsyn_n[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sort the hypernym synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in hypsyn_n:\n",
    "    i.sort()\n",
    "hypsyn_n.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'hood.n.01\", 'vicinity.n.01']\n",
      "['1530s.n.01', 'decade.n.01']\n",
      "['1750s.n.01', 'decade.n.01']\n",
      "['1760s.n.01', 'decade.n.01']\n",
      "['1770s.n.01', 'decade.n.01']\n",
      "['1780s.n.01', 'decade.n.01']\n",
      "['1790s.n.01', 'decade.n.01']\n",
      "['18-karat_gold.n.01', 'alloy.n.01']\n",
      "['1820s.n.01', 'decade.n.01']\n",
      "['1830s.n.01', 'decade.n.01']\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(hypsyn_n[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function \"get_hyppairs_n( )\" is used to get all hypernym pairs of an hypernym synset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_hyppairs_n(hypnym):\n",
    "    syn_1 = wordnet.synset(hypnym[0]).lemmas()\n",
    "    synset_1 = list()\n",
    "    for i in syn_1:\n",
    "        synset_1.append(str(i)[7:-2])\n",
    "    syn_2 = wordnet.synset(hypnym[1]).lemmas()\n",
    "    synset_2 = list()\n",
    "    for i in syn_2:\n",
    "        synset_2.append(str(i)[7:-2])\n",
    "    hyp_pair = list(itertools.product(synset_1, synset_2))\n",
    "    return hyp_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"'hood.n.01.'hood\", 'vicinity.n.01.vicinity')\n",
      "(\"'hood.n.01.'hood\", 'vicinity.n.01.locality')\n",
      "(\"'hood.n.01.'hood\", 'vicinity.n.01.neighborhood')\n",
      "(\"'hood.n.01.'hood\", 'vicinity.n.01.neighbourhood')\n",
      "(\"'hood.n.01.'hood\", 'vicinity.n.01.neck_of_the_woods')\n"
     ]
    }
   ],
   "source": [
    "for i in get_hyppairs_n([\"'hood.n.01\", 'vicinity.n.01']):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get all hypernym pairs for all hypernym synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hyppair_n = list()\n",
    "for i in hypsyn_n:\n",
    "    hyppair_n.append(get_hyppairs_n(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hyppairs_n = list()\n",
    "for i in hyppair_n:\n",
    "    for j in i:\n",
    "        hyppairs_n.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"'hood.n.01.'hood\", 'vicinity.n.01.vicinity')\n",
      "(\"'hood.n.01.'hood\", 'vicinity.n.01.locality')\n",
      "(\"'hood.n.01.'hood\", 'vicinity.n.01.neighborhood')\n",
      "(\"'hood.n.01.'hood\", 'vicinity.n.01.neighbourhood')\n",
      "(\"'hood.n.01.'hood\", 'vicinity.n.01.neck_of_the_woods')\n",
      "('1530s.n.01.1530s', 'decade.n.01.decade')\n",
      "('1530s.n.01.1530s', 'decade.n.01.decennary')\n",
      "('1530s.n.01.1530s', 'decade.n.01.decennium')\n",
      "('1750s.n.01.1750s', 'decade.n.01.decade')\n",
      "('1750s.n.01.1750s', 'decade.n.01.decennary')\n",
      "('1750s.n.01.1750s', 'decade.n.01.decennium')\n",
      "('1760s.n.01.1760s', 'decade.n.01.decade')\n",
      "('1760s.n.01.1760s', 'decade.n.01.decennary')\n",
      "('1760s.n.01.1760s', 'decade.n.01.decennium')\n",
      "('1770s.n.01.1770s', 'decade.n.01.decade')\n",
      "('1770s.n.01.1770s', 'decade.n.01.decennary')\n",
      "('1770s.n.01.1770s', 'decade.n.01.decennium')\n",
      "('1780s.n.01.1780s', 'decade.n.01.decade')\n",
      "('1780s.n.01.1780s', 'decade.n.01.decennary')\n",
      "('1780s.n.01.1780s', 'decade.n.01.decennium')\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(hyppairs_n[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261308\n"
     ]
    }
   ],
   "source": [
    "print(len(hyppairs_n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Verb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function \"get_hypernym_synset_v( )\" is used to get the hypernym synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_hypernym_synset_v(synset):\n",
    "    syn_hyp_set = list()\n",
    "    if wordnet.synset(synset).hypernyms():\n",
    "        for i in wordnet.synset(synset).hypernyms():\n",
    "            hypset = str(i)[8:-2]\n",
    "            hypernym_synset = [synset, hypset]\n",
    "            syn_hyp_set.append(hypernym_synset)\n",
    "            syn_hyp_set = list(syn_hyp_set for syn_hyp_set,_ in itertools.groupby(syn_hyp_set))\n",
    "    return syn_hyp_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get all verb hypernym synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hypernym_synset_v = list()\n",
    "for i in synsets_v:\n",
    "    hypernym_synset_pair = get_hypernym_synset_v(i)\n",
    "    if hypernym_synset_pair != []:\n",
    "        hypernym_synset_v.append(hypernym_synset_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hypsyn_v = list()\n",
    "for i in hypernym_synset_v:\n",
    "    for j in i:\n",
    "        hypsyn_v.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['respire.v.02', 'undergo.v.01']\n",
      "['respire.v.01', 'breathe.v.01']\n",
      "['choke.v.01', 'breathe.v.01']\n",
      "['hyperventilate.v.02', 'breathe.v.01']\n",
      "['hyperventilate.v.01', 'treat.v.03']\n",
      "['aspirate.v.03', 'inhale.v.02']\n",
      "['burp.v.01', 'emit.v.01']\n",
      "['force_out.v.08', 'emit.v.01']\n",
      "['hiccup.v.01', 'breathe.v.01']\n",
      "['sigh.v.01', 'breathe.v.01']\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(hypsyn_v[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sort the hypernym synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in hypsyn_v:\n",
    "    i.sort()\n",
    "hypsyn_v.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abacinate.v.01', 'blind.v.02']\n",
      "['abandon.v.01', 'chuck.v.02']\n",
      "['abandon.v.01', 'consign.v.01']\n",
      "['abandon.v.01', 'discard.v.01']\n",
      "['abandon.v.01', 'dispense_with.v.03']\n",
      "['abandon.v.01', 'forfeit.v.01']\n",
      "['abandon.v.02', 'foreswear.v.02']\n",
      "['abandon.v.05', 'ditch.v.01']\n",
      "['abandon.v.05', 'expose.v.09']\n",
      "['abandon.v.05', 'leave.v.02']\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(hypsyn_v[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function \"get_hyppairs_v( )\" is used to get all hypernym pairs of an hypernym synset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_hyppairs_v(hypnym):\n",
    "    syn_1 = wordnet.synset(hypnym[0]).lemmas()\n",
    "    synset_1 = list()\n",
    "    for i in syn_1:\n",
    "        synset_1.append(str(i)[7:-2])\n",
    "    syn_2 = wordnet.synset(hypnym[1]).lemmas()\n",
    "    synset_2 = list()\n",
    "    for i in syn_2:\n",
    "        synset_2.append(str(i)[7:-2])\n",
    "    hyp_pair = list(itertools.product(synset_1, synset_2))\n",
    "    return hyp_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('abacinate.v.01.abacinate', 'blind.v.02.blind')\n"
     ]
    }
   ],
   "source": [
    "for i in get_hyppairs_v(['abacinate.v.01', 'blind.v.02']):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get all hypernym pairs for all hypernym synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hyppair_v = list()\n",
    "for i in hypsyn_v:\n",
    "    hyppair_v.append(get_hyppairs_v(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hyppairs_v = list()\n",
    "for i in hyppair_v:\n",
    "    for j in i:\n",
    "        hyppairs_v.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('abacinate.v.01.abacinate', 'blind.v.02.blind')\n",
      "('abandon.v.01.abandon', 'chuck.v.02.chuck')\n",
      "('abandon.v.01.abandon', 'chuck.v.02.ditch')\n",
      "('abandon.v.01.abandon', 'consign.v.01.consign')\n",
      "('abandon.v.01.abandon', 'discard.v.01.discard')\n",
      "('abandon.v.01.abandon', 'discard.v.01.fling')\n",
      "('abandon.v.01.abandon', 'discard.v.01.toss')\n",
      "('abandon.v.01.abandon', 'discard.v.01.toss_out')\n",
      "('abandon.v.01.abandon', 'discard.v.01.toss_away')\n",
      "('abandon.v.01.abandon', 'discard.v.01.chuck_out')\n",
      "('abandon.v.01.abandon', 'discard.v.01.cast_aside')\n",
      "('abandon.v.01.abandon', 'discard.v.01.dispose')\n",
      "('abandon.v.01.abandon', 'discard.v.01.throw_out')\n",
      "('abandon.v.01.abandon', 'discard.v.01.cast_out')\n",
      "('abandon.v.01.abandon', 'discard.v.01.throw_away')\n",
      "('abandon.v.01.abandon', 'discard.v.01.cast_away')\n",
      "('abandon.v.01.abandon', 'discard.v.01.put_away')\n",
      "('abandon.v.01.abandon', 'dispense_with.v.03.dispense_with')\n",
      "('abandon.v.01.abandon', 'forfeit.v.01.forfeit')\n",
      "('abandon.v.01.abandon', 'forfeit.v.01.give_up')\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(hyppairs_v[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68088\n"
     ]
    }
   ],
   "source": [
    "print(len(hyppairs_v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Adjective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function \"get_hypernym_synset_a( )\" is used to get the hypernym synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_hypernym_synset_a(synset):\n",
    "    syn_hyp_set = list()\n",
    "    if wordnet.synset(synset).hypernyms():\n",
    "        for i in wordnet.synset(synset).hypernyms():\n",
    "            hypset = str(i)[8:-2]\n",
    "            hypernym_synset = [synset, hypset]\n",
    "            syn_hyp_set.append(hypernym_synset)\n",
    "            syn_hyp_set = list(syn_hyp_set for syn_hyp_set,_ in itertools.groupby(syn_hyp_set))\n",
    "    return syn_hyp_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get all adjective hypernym synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hypernym_synset_a = list()\n",
    "for i in synsets_a:\n",
    "    hypernym_synset_pair = get_hypernym_synset_a(i)\n",
    "    if hypernym_synset_pair != []:\n",
    "        hypernym_synset_a.append(hypernym_synset_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hypsyn_a = list()\n",
    "for i in hypernym_synset_a:\n",
    "    for j in i:\n",
    "        hypsyn_a.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(hypernym_synset_a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. The total amount of the hypernyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "329396\n"
     ]
    }
   ],
   "source": [
    "print(len(hyppairs_n)+len(hyppairs_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hyppairs = list()\n",
    "for i in hyppairs_n:\n",
    "    hyppairs.append(i)\n",
    "for j in hyppairs_v:\n",
    "    hyppairs.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('hyppairs.txt', 'w') as f:\n",
    "    for i in hyppairs:\n",
    "        f.write(\"%s\\n\" % str(i))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
